{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to load and pre-process data from the [COCO dataset](http://cocodataset.org/#home). You will also design a CNN-RNN model for automatically generating image captions.\n",
    "\n",
    "Note that **any amendments that you make to this notebook will not be graded**.  However, you will use the instructions provided in **Step 3** and **Step 4** to implement your own CNN encoder and RNN decoder by making amendments to the **models.py** file provided as part of this project.  Your **models.py** file **will be graded**. \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Explore the Data Loader\n",
    "- [Step 2](#step2): Use the Data Loader to Obtain Batches\n",
    "- [Step 3](#step3): Experiment with the CNN Encoder\n",
    "- [Step 4](#step4): Implement the RNN Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Explore the Data Loader\n",
    "\n",
    "We have already written a [data loader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) that you can use to load the COCO dataset in batches. \n",
    "\n",
    "In the code cell below, you will initialize the data loader by using the `get_loader` function in **data_loader.py**.  \n",
    "\n",
    "> For this project, you are not permitted to change the **data_loader.py** file, which must be used as-is.\n",
    "\n",
    "The `get_loader` function takes as input a number of arguments that can be explored in **data_loader.py**.  Take the time to explore these arguments now by opening **data_loader.py** in a new window.  Most of the arguments must be left at their default values, and you are only allowed to amend the values of the arguments below:\n",
    "1. **`transform`** - an [image transform](http://pytorch.org/docs/master/torchvision/transforms.html) specifying how to pre-process the images and convert them to PyTorch tensors before using them as input to the CNN encoder.  For now, you are encouraged to keep the transform as provided in `transform_train`.  You will have the opportunity later to choose your own image transform to pre-process the COCO images.\n",
    "2. **`mode`** - one of `'train'` (loads the training data in batches) or `'test'` (for the test data). We will say that the data loader is in training or test mode, respectively.  While following the instructions in this notebook, please keep the data loader in training mode by setting `mode='train'`.\n",
    "3. **`batch_size`** - determines the batch size.  When training the model, this is number of image-caption pairs used to amend the model weights in each training step.\n",
    "4. **`vocab_threshold`** - the total number of times that a word must appear in the in the training captions before it is used as part of the vocabulary.  Words that have fewer than `vocab_threshold` occurrences in the training captions are considered unknown words. \n",
    "5. **`vocab_from_file`** - a Boolean that decides whether to load the vocabulary from file.  \n",
    "\n",
    "We will describe the `vocab_threshold` and `vocab_from_file` arguments in more detail soon.  For now, run the code cell below.  Be patient - it may take a couple of minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/doz007/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "from pycocotools.coco import COCO\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations file:  /datasets/ee285f-public/COCO-Annotations/annotations_trainval2014/captions_train2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.90s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 511/414113 [00:00<01:21, 5103.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:57<00:00, 7151.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 10\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 8856\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations file:  /datasets/ee285f-public/COCO-Annotations/annotations_trainval2014/captions_train2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.85s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.77s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 605/414113 [00:00<01:08, 6045.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:57<00:00, 7186.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Modify the minimum word count threshold.\n",
    "vocab_threshold = 4\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 9955\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special unknown word: <unk>\n",
      "All unknown words are mapped to this integer: 2\n"
     ]
    }
   ],
   "source": [
    "unk_word = data_loader.dataset.vocab.unk_word\n",
    "print('Special unknown word:', unk_word)\n",
    "\n",
    "print('All unknown words are mapped to this integer:', data_loader.dataset.vocab(unk_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this for yourself below, by pre-processing the provided nonsense words that never appear in the training captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.dataset.vocab('jfkafejw'))\n",
    "print(data_loader.dataset.vocab('ieowoqjf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.82s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 590/414113 [00:00<01:10, 5899.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:53<00:00, 7808.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the data loader (from file). Note that it runs much faster than before!\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, you will learn how to use the data loader to obtain batches of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Use the Data Loader to Obtain Batches\n",
    "\n",
    "The captions in the dataset vary greatly in length.  You can see this by examining `data_loader.dataset.caption_lengths`, a Python list with one entry for each training caption (where the value stores the length of the corresponding caption).  \n",
    "\n",
    "In the code cell below, we use this list to print the total number of captions in the training data with each length.  As you will see below, the majority of captions have length 10.  Likewise, very short and very long captions are quite rare.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 10 --- count: 86332\n",
      "value: 11 --- count: 79945\n",
      "value:  9 --- count: 71935\n",
      "value: 12 --- count: 57639\n",
      "value: 13 --- count: 37648\n",
      "value: 14 --- count: 22335\n",
      "value:  8 --- count: 20769\n",
      "value: 15 --- count: 12842\n",
      "value: 16 --- count:  7729\n",
      "value: 17 --- count:  4842\n",
      "value: 18 --- count:  3103\n",
      "value: 19 --- count:  2015\n",
      "value:  7 --- count:  1597\n",
      "value: 20 --- count:  1451\n",
      "value: 21 --- count:   999\n",
      "value: 22 --- count:   683\n",
      "value: 23 --- count:   534\n",
      "value: 24 --- count:   383\n",
      "value: 25 --- count:   277\n",
      "value: 26 --- count:   215\n",
      "value: 27 --- count:   159\n",
      "value: 28 --- count:   115\n",
      "value: 29 --- count:    86\n",
      "value: 30 --- count:    58\n",
      "value: 31 --- count:    49\n",
      "value: 32 --- count:    44\n",
      "value: 34 --- count:    39\n",
      "value: 37 --- count:    32\n",
      "value: 33 --- count:    31\n",
      "value: 35 --- count:    31\n",
      "value: 36 --- count:    26\n",
      "value: 38 --- count:    18\n",
      "value: 39 --- count:    18\n",
      "value: 43 --- count:    16\n",
      "value: 44 --- count:    16\n",
      "value: 48 --- count:    12\n",
      "value: 45 --- count:    11\n",
      "value: 42 --- count:    10\n",
      "value: 40 --- count:     9\n",
      "value: 49 --- count:     9\n",
      "value: 46 --- count:     9\n",
      "value: 47 --- count:     7\n",
      "value: 50 --- count:     6\n",
      "value: 51 --- count:     6\n",
      "value: 41 --- count:     6\n",
      "value: 52 --- count:     5\n",
      "value: 54 --- count:     3\n",
      "value: 56 --- count:     2\n",
      "value:  6 --- count:     2\n",
      "value: 53 --- count:     2\n",
      "value: 55 --- count:     2\n",
      "value: 57 --- count:     1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tally the total number of training captions with each length.\n",
    "counter = Counter(data_loader.dataset.caption_lengths)\n",
    "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "for value, count in lengths:\n",
    "    print('value: %2d --- count: %5d' % (value, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled indices: [221787, 353727, 350241, 5854, 368426, 82739, 185482, 352657, 265917, 385444]\n",
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 15])\n",
      "images: tensor([[[[ 0.6392,  0.6563,  0.6734,  ...,  1.5468,  1.5468,  1.5639],\n",
      "          [ 0.6563,  0.6734,  0.6906,  ...,  1.5297,  1.5297,  1.5297],\n",
      "          [ 0.6734,  0.6906,  0.7077,  ...,  1.5125,  1.4954,  1.4954],\n",
      "          ...,\n",
      "          [ 1.0159,  0.9646,  0.9988,  ...,  2.0092,  2.0092,  2.0777],\n",
      "          [ 0.9474,  0.9646,  1.0331,  ...,  2.0092,  2.0948,  2.0777],\n",
      "          [ 0.9817,  1.0502,  1.0673,  ...,  2.0434,  2.0092,  2.0948]],\n",
      "\n",
      "         [[ 0.3627,  0.3803,  0.3978,  ...,  1.3431,  1.3431,  1.3431],\n",
      "          [ 0.3803,  0.3978,  0.3978,  ...,  1.3431,  1.3431,  1.3431],\n",
      "          [ 0.3627,  0.3803,  0.3978,  ...,  1.3256,  1.3081,  1.3081],\n",
      "          ...,\n",
      "          [ 0.8354,  0.7829,  0.8179,  ...,  1.9559,  1.9559,  2.0259],\n",
      "          [ 0.7654,  0.7829,  0.8529,  ...,  1.9559,  2.0434,  2.0259],\n",
      "          [ 0.8004,  0.8704,  0.8880,  ...,  1.9909,  1.9559,  2.0434]],\n",
      "\n",
      "         [[ 0.1651,  0.1825,  0.1999,  ...,  1.4374,  1.4374,  1.4374],\n",
      "          [ 0.1825,  0.1999,  0.1999,  ...,  1.4374,  1.4374,  1.4374],\n",
      "          [ 0.1825,  0.1999,  0.2173,  ...,  1.4200,  1.4025,  1.4025],\n",
      "          ...,\n",
      "          [ 0.7576,  0.7054,  0.7402,  ...,  1.8905,  1.8905,  1.9603],\n",
      "          [ 0.6879,  0.7054,  0.7751,  ...,  1.8905,  1.9777,  1.9603],\n",
      "          [ 0.7228,  0.7925,  0.8099,  ...,  1.9254,  1.8905,  1.9777]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1768,  0.1254, -0.0116,  ...,  0.5364,  0.5364,  0.5022],\n",
      "          [ 0.2967,  0.3309,  0.2111,  ...,  0.6906,  0.7077,  0.6906],\n",
      "          [ 0.3994,  0.4337,  0.3823,  ...,  0.7591,  0.7419,  0.7591],\n",
      "          ...,\n",
      "          [ 0.3823,  0.2282,  0.2967,  ...,  0.5022,  0.5707,  0.5022],\n",
      "          [ 0.4166,  0.4679,  0.4851,  ...,  0.4166,  0.4851,  0.4337],\n",
      "          [ 0.2111,  0.2624,  0.4337,  ...,  0.4337,  0.4679,  0.5193]],\n",
      "\n",
      "         [[ 0.0126, -0.0574, -0.1625,  ...,  0.2752,  0.3102,  0.2927],\n",
      "          [ 0.0476,  0.0826,  0.0126,  ...,  0.3627,  0.4153,  0.4153],\n",
      "          [ 0.1702,  0.2227,  0.2402,  ...,  0.5028,  0.4503,  0.5028],\n",
      "          ...,\n",
      "          [ 0.0826, -0.0399,  0.0301,  ...,  0.2052,  0.2577,  0.2052],\n",
      "          [ 0.1527,  0.2577,  0.2577,  ...,  0.1176,  0.1877,  0.1702],\n",
      "          [-0.0749, -0.0049,  0.2052,  ...,  0.1877,  0.2577,  0.2927]],\n",
      "\n",
      "         [[-0.2881, -0.3055, -0.4101,  ..., -0.0267, -0.0092, -0.0441],\n",
      "          [-0.2881, -0.2358, -0.2532,  ...,  0.0256,  0.0779,  0.0953],\n",
      "          [-0.1312, -0.1312, -0.1312,  ...,  0.0779,  0.0605,  0.0953],\n",
      "          ...,\n",
      "          [-0.2532, -0.3753, -0.3404,  ..., -0.1661, -0.0790, -0.1138],\n",
      "          [-0.2532, -0.2532, -0.1661,  ..., -0.1661, -0.1312, -0.1312],\n",
      "          [-0.3927, -0.3578, -0.2532,  ..., -0.2358, -0.2184, -0.2184]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4440,  1.4269,  1.4440,  ...,  1.4269,  1.4269,  1.4269],\n",
      "          [ 1.4269,  1.4098,  1.4269,  ...,  1.4269,  1.4269,  1.4098],\n",
      "          [ 1.4269,  1.4269,  1.4440,  ...,  1.4269,  1.4269,  1.4269],\n",
      "          ...,\n",
      "          [ 0.9817,  0.6734,  0.0741,  ...,  0.1597, -0.1486, -0.0287],\n",
      "          [ 0.9988,  1.0502,  0.7248,  ...,  0.6049,  0.5707,  0.6221],\n",
      "          [ 1.3413,  1.3584,  1.3927,  ...,  0.4679,  0.9817,  1.0673]],\n",
      "\n",
      "         [[ 1.5532,  1.5357,  1.5532,  ...,  1.5357,  1.5532,  1.5532],\n",
      "          [ 1.5357,  1.5182,  1.5357,  ...,  1.5357,  1.5532,  1.5357],\n",
      "          [ 1.5357,  1.5357,  1.5532,  ...,  1.5357,  1.5357,  1.5707],\n",
      "          ...,\n",
      "          [ 0.5203,  0.1877, -0.3901,  ..., -0.3200, -0.6702, -0.5301],\n",
      "          [ 0.4503,  0.4678,  0.1176,  ..., -0.0049, -0.0399,  0.0301],\n",
      "          [ 0.8704,  0.8529,  0.8880,  ..., -0.1099,  0.3803,  0.4503]],\n",
      "\n",
      "         [[ 1.9428,  1.9080,  1.9254,  ...,  1.9428,  1.9603,  1.9603],\n",
      "          [ 1.9254,  1.8905,  1.9254,  ...,  1.9428,  1.9603,  1.9428],\n",
      "          [ 1.9428,  1.9080,  1.9254,  ...,  1.9428,  1.9428,  1.9777],\n",
      "          ...,\n",
      "          [ 0.3393, -0.0092, -0.7064,  ..., -0.6890, -1.0027, -0.8807],\n",
      "          [ 0.1999,  0.1999, -0.1487,  ..., -0.3404, -0.3927, -0.3230],\n",
      "          [ 0.7576,  0.6531,  0.6356,  ..., -0.3753, -0.0267,  0.1302]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8335, -0.8335, -0.2513,  ..., -0.4054, -0.4739, -0.4911],\n",
      "          [-0.7822, -0.7993, -0.1314,  ..., -0.3712, -0.3883, -0.2684],\n",
      "          [-0.7308, -0.6965, -0.1143,  ..., -0.4568, -0.4226, -0.3712],\n",
      "          ...,\n",
      "          [-0.2342, -0.3027, -0.2856,  ..., -1.5870, -1.5870, -1.5870],\n",
      "          [-0.6452, -0.2513, -0.2342,  ..., -1.6042, -1.5870, -1.6042],\n",
      "          [-1.1589, -0.8678, -0.5424,  ..., -1.6213, -1.5870, -1.6042]],\n",
      "\n",
      "         [[-0.1625, -0.0574,  0.0301,  ..., -0.3725, -0.4251, -0.4426],\n",
      "          [ 0.0126,  0.0126,  0.2577,  ..., -0.3200, -0.3025, -0.2150],\n",
      "          [-0.0049,  0.0301,  0.2402,  ..., -0.4076, -0.3375, -0.3200],\n",
      "          ...,\n",
      "          [-0.1800, -0.2325, -0.1800,  ..., -1.5805, -1.5805, -1.5980],\n",
      "          [-0.6001, -0.1800, -0.0924,  ..., -1.5980, -1.5805, -1.5980],\n",
      "          [-1.1429, -0.8277, -0.4426,  ..., -1.5980, -1.5630, -1.5805]],\n",
      "\n",
      "         [[ 0.2696,  0.3568,  0.1651,  ..., -0.4798, -0.5321, -0.5495],\n",
      "          [ 0.3916,  0.3393,  0.0779,  ..., -0.4450, -0.4450, -0.3404],\n",
      "          [ 0.2871,  0.2871, -0.1661,  ..., -0.5321, -0.4798, -0.4450],\n",
      "          ...,\n",
      "          [-0.4101, -0.4101, -0.3578,  ..., -1.3861, -1.3861, -1.3861],\n",
      "          [-0.7936, -0.3230, -0.2707,  ..., -1.3861, -1.3687, -1.4036],\n",
      "          [-1.2641, -0.9678, -0.6018,  ..., -1.3861, -1.3513, -1.3687]]],\n",
      "\n",
      "\n",
      "        [[[-1.0904, -0.9705, -1.1932,  ..., -0.7479, -0.7822, -0.8164],\n",
      "          [-0.7479, -0.7308, -0.8164,  ..., -0.7479, -0.7822, -0.7479],\n",
      "          [-0.7137, -0.7479, -0.7993,  ..., -0.7479, -0.7822, -0.7137],\n",
      "          ...,\n",
      "          [-1.6042, -1.5528, -1.5357,  ..., -0.9192, -0.9705, -0.9877],\n",
      "          [-1.3987, -1.3644, -1.3987,  ..., -0.7993, -0.8335, -0.8164],\n",
      "          [-1.4329, -1.1932, -1.2617,  ..., -0.7822, -0.8164, -0.7650]],\n",
      "\n",
      "         [[-0.9678, -0.8627, -1.1253,  ..., -0.6702, -0.7052, -0.7402],\n",
      "          [-0.6176, -0.6176, -0.7227,  ..., -0.6702, -0.7052, -0.6702],\n",
      "          [-0.5826, -0.6176, -0.7052,  ..., -0.6527, -0.7052, -0.6527],\n",
      "          ...,\n",
      "          [-1.7556, -1.7206, -1.7731,  ..., -1.4230, -1.4930, -1.4580],\n",
      "          [-1.6155, -1.5805, -1.7206,  ..., -1.4055, -1.4405, -1.3880],\n",
      "          [-1.6856, -1.4755, -1.6331,  ..., -1.4580, -1.4755, -1.4230]],\n",
      "\n",
      "         [[-0.9853, -0.8807, -1.1247,  ..., -0.6541, -0.6890, -0.7238],\n",
      "          [-0.6715, -0.6715, -0.7936,  ..., -0.6541, -0.6890, -0.6541],\n",
      "          [-0.6541, -0.7413, -0.8110,  ..., -0.6541, -0.7064, -0.6367],\n",
      "          ...,\n",
      "          [-1.6127, -1.5256, -1.6127,  ..., -1.7347, -1.7347, -1.6824],\n",
      "          [-1.4384, -1.3861, -1.5081,  ..., -1.6999, -1.6999, -1.6302],\n",
      "          [-1.5081, -1.2816, -1.4036,  ..., -1.7522, -1.7347, -1.6650]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6906,  1.0331,  1.2899,  ..., -1.7412, -1.7240, -1.7240],\n",
      "          [ 0.6392,  1.0159,  1.2899,  ..., -1.6898, -1.6727, -1.7412],\n",
      "          [ 0.8104,  0.9988,  1.2214,  ..., -1.7583, -1.7240, -1.7069],\n",
      "          ...,\n",
      "          [ 1.7865,  1.8037,  1.7352,  ..., -2.1008, -2.1008, -2.0837],\n",
      "          [ 1.5810,  1.1529,  0.9474,  ..., -2.0837, -2.0837, -2.1179],\n",
      "          [ 0.7248,  0.3138,  0.2111,  ..., -2.0837, -2.1008, -2.1008]],\n",
      "\n",
      "         [[ 0.0476,  0.2577,  0.4853,  ..., -1.7556, -1.7206, -1.7206],\n",
      "          [-0.0224,  0.2577,  0.4503,  ..., -1.7206, -1.7031, -1.7206],\n",
      "          [ 0.0651,  0.1877,  0.3627,  ..., -1.7381, -1.7206, -1.6856],\n",
      "          ...,\n",
      "          [ 1.2731,  1.0280,  1.0455,  ..., -2.0182, -2.0182, -2.0007],\n",
      "          [ 0.8529,  0.5553,  0.4678,  ..., -2.0007, -2.0007, -2.0357],\n",
      "          [ 0.2402,  0.1001,  0.0476,  ..., -2.0007, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-0.1312,  0.0605,  0.3219,  ..., -1.5779, -1.5604, -1.5779],\n",
      "          [-0.2010,  0.0605,  0.2173,  ..., -1.5430, -1.5604, -1.5779],\n",
      "          [-0.1138, -0.0092,  0.0953,  ..., -1.5779, -1.5604, -1.5430],\n",
      "          ...,\n",
      "          [ 0.4614,  0.2696,  0.3219,  ..., -1.7870, -1.7870, -1.7696],\n",
      "          [ 0.1651, -0.0615, -0.0964,  ..., -1.7696, -1.7696, -1.8044],\n",
      "          [-0.1835, -0.3055, -0.2881,  ..., -1.7696, -1.7870, -1.7870]]]])\n",
      "captions: tensor([[   0,    3,  112,   86,  615,   86, 1462,    6,  233, 3593,   32,  861,\n",
      "           28,   18,    1],\n",
      "        [   0,    3, 1016, 3104,  103,  224,   77,   32,  392,   13,    3, 1997,\n",
      "          204,   18,    1],\n",
      "        [   0,   80,   13, 2755,  131,  364,  161,   72,  409,   77,   47,  327,\n",
      "          204,   18,    1],\n",
      "        [   0,    3, 4914, 7365,   86, 2946,    6,    3, 2603,   39,    3,  449,\n",
      "          821,   18,    1],\n",
      "        [   0,    3,  267, 1912,   13,   50, 3072,  873, 1334,   77,  121,   13,\n",
      "         3115,   18,    1],\n",
      "        [   0,  145,   10,  105,    3,  216, 1236,  564,    6,    3,   20,   56,\n",
      "            6,   57,    1],\n",
      "        [   0,    3,   98,  139,    3,  140,   39,    3,  185,   93,  285,  565,\n",
      "          717,   18,    1],\n",
      "        [   0,    3,   91,  505,  176,  263,   92,  877,  160,  855,   39,   32,\n",
      "           57,   18,    1],\n",
      "        [   0,    3,  861,   28,  105,    3,  715,   86,  112,   86,    6,  197,\n",
      "          626,   18,    1],\n",
      "        [   0,    3,  169,  491,    3,    2, 2354,  248,   13,    3,  608,   13,\n",
      "         2076,   18,    1]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)\n",
    "\n",
    "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Obtain the batch.\n",
    "images, captions = next(iter(data_loader))\n",
    "    \n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)\n",
    "\n",
    "# (Optional) Uncomment the lines of code below to print the pre-processed images and captions.\n",
    "print('images:', images)\n",
    "print('captions:', captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Experiment with the CNN Encoder\n",
    "\n",
    "Run the code cell below to import `EncoderCNN` and `DecoderRNN` from **model.py**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for any changes in model.py, and re-load it automatically.\n",
    "# % load_ext autoreload\n",
    "# % autoreload 2\n",
    "\n",
    "# Import EncoderCNN and DecoderRNN. \n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['model'])\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell we define a `device` that you will use move PyTorch tensors to GPU (if CUDA is available).  Run this code cell before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to instantiate the CNN encoder in `encoder`.  \n",
    "\n",
    "The pre-processed images from the batch in **Step 2** of this notebook are then passed through the encoder, and the output is stored in `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /tmp/xdg-cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
      "100%|██████████| 230M/230M [00:06<00:00, 38.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(features): <class 'torch.Tensor'>\n",
      "features.shape: torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Specify the dimensionality of the image embedding.\n",
    "embed_size = 256\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Initialize the encoder. (Optional: Add additional arguments if necessary.)\n",
    "encoder = EncoderCNN(embed_size)\n",
    "\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "encoder.to(device)\n",
    "    \n",
    "# Move last batch of images (from Step 2) to GPU if CUDA is available.   \n",
    "images = images.to(device)\n",
    "\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "# Check that your encoder satisfies some requirements of the project! :D\n",
    "assert type(features)==torch.Tensor, \"Encoder output needs to be a PyTorch Tensor.\" \n",
    "assert (features.shape[0]==batch_size) & (features.shape[1]==embed_size), \"The shape of the encoder output is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder that we provide to you uses the pre-trained ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images.  The output is then flattened to a vector, before being passed through a `Linear` layer to transform the feature vector to have the same size as the word embedding.\n",
    "\n",
    "![Encoder](images/encoder.png)\n",
    "\n",
    "You are welcome (and encouraged) to amend the encoder in **model.py**, to experiment with other architectures.  In particular, consider using a [different pre-trained model architecture](http://pytorch.org/docs/master/torchvision/models.html).  You may also like to [add batch normalization](http://pytorch.org/docs/master/nn.html#normalization-layers).  \n",
    "\n",
    "> You are **not** required to change anything about the encoder.\n",
    "\n",
    "For this project, you **must** incorporate a pre-trained CNN into your encoder.  Your `EncoderCNN` class must take `embed_size` as an input argument, which will also correspond to the dimensionality of the input to the RNN decoder that you will implement in Step 4.  When you train your model in the next notebook in this sequence (**2_Training.ipynb**), you are welcome to tweak the value of `embed_size`.\n",
    "\n",
    "If you decide to modify the `EncoderCNN` class, save **model.py** and re-execute the code cell above.  If the code cell returns an assertion error, then please follow the instructions to modify your code before proceeding.  The assert statements ensure that `features` is a PyTorch tensor with shape `[batch_size, embed_size]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Implement the RNN Decoder\n",
    "\n",
    "Before executing the next code cell, you must write `__init__` and `forward` methods in the `DecoderRNN` class in **model.py**.  (Do **not** write the `sample` method yet - you will work with this method when you reach **3_Inference.ipynb**.)\n",
    "\n",
    "> The `__init__` and `forward` methods in the `DecoderRNN` class are the only things that you **need** to modify as part of this notebook.  You will write more implementations in the notebooks that appear later in the sequence.\n",
    "\n",
    "Your decoder will be an instance of the `DecoderRNN` class and must accept as input:\n",
    "- the PyTorch tensor `features` containing the embedded image features (outputted in Step 3, when the last batch of images from Step 2 was passed through `encoder`), along with\n",
    "- a PyTorch tensor corresponding to the last batch of captions (`captions`) from Step 2.\n",
    "\n",
    "Note that the way we have written the data loader should simplify your code a bit.  In particular, every training batch will contain pre-processed captions where all have the same length (`captions.shape[1]`), so **you do not need to worry about padding**.  \n",
    "> While you are encouraged to implement the decoder described in [this paper](https://arxiv.org/pdf/1411.4555.pdf), you are welcome to implement any architecture of your choosing, as long as it uses at least one RNN layer, with hidden dimension `hidden_size`.  \n",
    "\n",
    "Although you will test the decoder using the last batch that is currently stored in the notebook, your decoder should be written to accept an arbitrary batch (of embedded image features and pre-processed captions [where all captions have the same length]) as input.  \n",
    "\n",
    "![Decoder](images/decoder.png)\n",
    "\n",
    "In the code cell below, `outputs` should be a PyTorch tensor with size `[batch_size, captions.shape[1], vocab_size]`.  Your output should be designed such that `outputs[i,j,k]` contains the model's predicted score, indicating how likely the `j`-th token in the `i`-th caption in the batch is the `k`-th token in the vocabulary.  In the next notebook of the sequence (**2_Training.ipynb**), we provide code to supply these scores to the [`torch.nn.CrossEntropyLoss`](http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss) optimizer in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(outputs): <class 'torch.Tensor'>\n",
      "outputs.shape: torch.Size([10, 15, 9955])\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of features in the hidden state of the RNN decoder.\n",
    "hidden_size = 512\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Store the size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the decoder.\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move the decoder to GPU if CUDA is available.\n",
    "decoder.to(device)\n",
    "    \n",
    "# Move last batch of captions (from Step 1) to GPU if CUDA is available \n",
    "captions = captions.to(device)\n",
    "\n",
    "# Pass the encoder output and captions through the decoder.\n",
    "outputs = decoder(features, captions)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)\n",
    "\n",
    "# Check that your decoder satisfies some requirements of the project! :D\n",
    "assert type(outputs)==torch.Tensor, \"Decoder output needs to be a PyTorch Tensor.\"\n",
    "assert (outputs.shape[0]==batch_size) & (outputs.shape[1]==captions.shape[1]) & (outputs.shape[2]==vocab_size), \"The shape of the decoder output is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you train your model in the next notebook in this sequence (**2_Training.ipynb**), you are welcome to tweak the value of `hidden_size`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
